{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDs:\n",
    "Insert yours IDs to the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID #1:\n",
    "\n",
    "ID #2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. You are free to add cells.\n",
    "1. Write your functions and your answers in this jupyter notebook only.\n",
    "1. Answers to theoretical questions should be written in **markdown cells (with $\\LaTeX$ support)**.\n",
    "1. Submit this jupyter notebook only using your ID as a filename. Not to use ZIP or RAR. For example, your Moodle submission file name should look like this (two id numbers): `123456789_987654321.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Defective products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a manufacturing pipeline products are 3% defective. We are interested in examining a defective product to see what goes wrong on the belt. We need to ask the facility manager to send us a set of independent samples for examination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.A\n",
    "\n",
    "How many independent samples should we ask for in order to have a 85% probability of having at least one defective product in the batch sent? You should write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "def binom(N, k, p):\n",
    "    return comb(N, k) * p ** k * (1 - p) ** (N - k)\n",
    "\n",
    "def confidence(N, p, d):\n",
    "    return 1 - sum([binom(N, i, p) for i in range(d)])\n",
    "\n",
    "def find_minimal_trials_count(d, p, P, history=False):\n",
    "    \"\"\"\n",
    "    d - number of at least desired defective products\n",
    "    p - probability of defectiveness\n",
    "    P - Desired confidence\n",
    "    \"\"\"\n",
    "    N = d\n",
    "    \n",
    "    results = []\n",
    "    while True:\n",
    "        c = confidence(N, p, d)\n",
    "        results.append((N, c))\n",
    "        if c >= P:\n",
    "            break\n",
    "        N += 1\n",
    "\n",
    "    if history:\n",
    "        return N, np.asarray(results)\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_minimal_trials_count(1, 0.03, 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.B\n",
    "Answer this part again with the following changes: products are 4% defective and we want a 95% probability of at least one defective product in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_minimal_trials_count(1, 0.04, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.C \n",
    "\n",
    "Consider the following cases and calculate how many independent samples are required: \n",
    "\n",
    "1. Products are 10% defective and we want a 90% probability of at least 5 defective products in the batch.\n",
    "1. Products are 30% defective and we want a 90% probability of at least 15 defective products in the batch.\n",
    "\n",
    "Explain the difference between the two results. You should use mathematical reasoning based on the properties of distributions you saw in class and visualizations in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_minimal_trials_count(5, 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_minimal_trials_count(15, 0.3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, res1 = find_minimal_trials_count(5, 0.1, 0.9, True)\n",
    "res1[:,0] -= 5\n",
    "_, res2 = find_minimal_trials_count(15, 0.3, 0.9, True)\n",
    "res2[:,0] -= 15\n",
    "\n",
    "plt.plot(*res1.T, label=\"d=5 p=0.1\")\n",
    "plt.plot(*res2.T, label=\"d=15 p=0.3\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Mathematic reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Rent distributions in Randomistan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of Randomistan conducted a survey to study the distribution of rent paid in two neighboring towns, Stochastic Heights and Random Grove, to be denoted SH and RG.<br> \n",
    "\n",
    "Here are some findings of the survey:\n",
    "* The population of SH and RG is 16,000 and 22,000 respectively. <br>\n",
    "* The mean rent in SH and RG is 6300RCU and 4200RCU respectively.\n",
    "* The median rent is 4600RCU in both towns.\n",
    "* The IQR of the rent is smaller in SH than in RG.\n",
    "\n",
    "All data generated in this question needs to be consistent with these findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.A\n",
    "Draw histograms that describe 2 different scenarii of possible distributions of rent in the two towns.\u000b",
    "Your histograms should:<br>\n",
    "* Use bins of 100RCU each.\n",
    "* Have at least 10 non zero bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "from numpy.random import uniform\n",
    "\n",
    "# Population\n",
    "pop_SH, pop_RG = 16000, 22000\n",
    "# Rent\n",
    "Œº_SH, Œº_RG = 6300, 4200\n",
    "med_SH, med_RG = 4600, 4600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def fix_dist(D, mean, median):\n",
    "    # Fix the mean\n",
    "    mean_diff = mean - D.mean()\n",
    "    D += mean_diff\n",
    "    # Fix the median, This assumes data is already splitted s.t \n",
    "    # half of it is smaller than the median and the other half bigger\n",
    "    D = np.concatenate([D, [median, median]])\n",
    "    D.sort()\n",
    "    # Fixing the mean in regard to the new added medians\n",
    "    median_diff = mean - median\n",
    "    if np.sign(median_diff) > 0:\n",
    "        D[-1] += 2 * median_diff\n",
    "    else:\n",
    "        D[0] += 2 * median_diff\n",
    "    return D\n",
    "\n",
    "def test_dist(dist):\n",
    "    print(\"Mean: %.2f\" % dist.mean())\n",
    "    print(\"Median: %.2f\" % np.median(dist))    \n",
    "    print(\"IQR: %.2f\" % iqr(dist))\n",
    "    print(\"Variance: %.2f\" % np.var(dist))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senario 1 - Two Uniform Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uni(mean, med, offset, width, size):\n",
    "    D1 = uniform(mean + offset, mean + offset + width, int(size/2) - 1) \n",
    "    D2 = uniform(mean - offset, mean - offset - width, int(size/2) - 1)\n",
    "    D = np.concatenate([D1, D2])\n",
    "    D = fix_dist(D, mean, med)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_SH = generate_uni(Œº_SH, med_SH, 1800, 500, pop_SH)\n",
    "test_dist(dist_SH)\n",
    "sns.histplot(dist_SH, binwidth=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_RG = generate_uni(Œº_RG, med_RG, 1800, 900, pop_RG)\n",
    "test_dist(dist_RG)\n",
    "sns.histplot(dist_RG, binwidth=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "senario 2 - Two Normal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal(mean, med, offset, sigma, size):\n",
    "    D1 = normal(mean + offset, sigma, int(size/2) - 1) \n",
    "    D2 = normal(mean - offset, sigma, int(size/2) - 1)\n",
    "    D = np.concatenate([D1, D2])\n",
    "    D = fix_dist(D, mean, med)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_SH = generate_normal(Œº_SH, med_SH, 2100, 100, pop_SH)\n",
    "test_dist(dist_SH)\n",
    "sns.histplot(dist_SH, binwidth=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_RG = generate_normal(Œº_RG, med_RG, 2500, 400, pop_RG)\n",
    "test_dist(dist_RG)\n",
    "sns.histplot(dist_RG, binwidth=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.B\n",
    "Draw a histogram of a third scenario with the same properties. <br>\n",
    "In addition, in this scenario the rent in SH should have a higher variance than the rent in RG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uni_quarters(mean, med, width1, width2, size):\n",
    "    offset = np.abs(mean - med) + 100\n",
    "    quarter_size = int(size/4)\n",
    "    D1 = uniform(mean - offset - width1, mean - offset - width1 - width2, quarter_size)\n",
    "    D2 = uniform(mean - offset, mean - offset - width1, quarter_size - 1)\n",
    "    D3 = uniform(mean + offset, mean + offset + width1, quarter_size - 1) \n",
    "    D4 = uniform(mean + offset + width1, mean + offset + width1 + width2, quarter_size)\n",
    "    D = np.concatenate([D1, D2, D3, D4])\n",
    "    D = fix_dist(D, mean, med)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_SH = generate_uni_quarters(Œº_SH, med_SH, 200, 2000, pop_SH)\n",
    "\n",
    "test_dist(dist_SH)\n",
    "sns.histplot(dist_SH, binwidth=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_RG = generate_uni_quarters(Œº_RG, med_RG, 2000, 200, pop_RG)\n",
    "test_dist(dist_RG)\n",
    "sns.histplot(dist_RG, binwidth=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survey also examined the per household income (PHI) in these two places.<br>\n",
    "\n",
    "It found that:<br>\n",
    "* The mean of PHI in SH is 12500 and in RG is 8500.\n",
    "* The median is 12000 in SH and 8000 in RG.\n",
    "* The covariance of the rent and the PHI was observed to be as in the formula below with $\\alpha=97\\%$ and $\\alpha=89\\%$ in SH and in RG respectively.<br><br>\n",
    "$$Cov(rent, PHI) = \\alpha * \\sqrt{Var(rent)} * \\sqrt{Var(PHI)}$$\n",
    "\n",
    "#### 2.C\n",
    "Produce rent and PHI data for the two cities, that is consistent with these findings. The covariances in your data can deviate by up to 1% from the numbers given $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Œº_PHI_SH = 12500\n",
    "Œº_PHI_RG = 8500\n",
    "med_PHI_SH = 12000\n",
    "med_PHI_RG = 8000\n",
    "ùõº_SH = 0.97\n",
    "ùõº_RG = 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_SH = generate_uni_quarters(Œº_PHI_SH, med_PHI_SH, 200, 2000, pop_SH)\n",
    "test_dist(PHI_SH)\n",
    "sns.histplot(PHI_SH, binwidth=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr(D1, D2):\n",
    "    cov_mat = np.cov(D1, D2)\n",
    "    cov = cov_mat[0][1]\n",
    "    # Variance calculation using np.var is slightly different than cov_mat result\n",
    "    corr = cov / (np.sqrt(np.var(D1))*np.sqrt(np.var(D2)))\n",
    "    #corr = cov / (np.sqrt(cov_mat[0][0])*np.sqrt(cov_mat[1][1]))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(dist_SH, PHI_SH)[0,1])\n",
    "print(calc_corr(dist_SH, PHI_SH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_RG = generate_uni_quarters(Œº_PHI_RG, med_PHI_RG, 100, 3500, pop_RG)\n",
    "test_dist(PHI_RG)\n",
    "sns.histplot(PHI_RG, binwidth=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(dist_RG, PHI_RG)[0,1])\n",
    "print(calc_corr(dist_RG, PHI_RG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.D\n",
    "Produce two heatmaps that describe these two bivariate joint distributions. Make sure you carefully consider the selected binning resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using jointplot fixes binning resolution automaticaly\n",
    "sns.jointplot(dist_SH, PHI_SH,kind=\"hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(dist_RG, PHI_RG, kind=\"hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Multinomial Distributions\n",
    "\n",
    "1. Let $X \\sim Multinomial(n,\\vec{p})$ be a multinomial random variable where $n=20$ and $\\vec{p} = (0.2,  0.1,  0.1,  0.1,  0.2,  0.3)$. Note that X is a vector of counts.\n",
    "\n",
    "\n",
    "2. Let $Y = X_2 + X_3 + X_4$ be a random variable.\n",
    "\n",
    "\n",
    "3. Create $k=100$ experiments where $X$ is sampled using Python. Calculate the empirical centralized third moment of $Y$ based on your $k$ experiments.\n",
    "\n",
    "\n",
    "4. Compare your result to the calculation in class for the centralized third moment of the **binomial** distribution and explain your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "from scipy.stats import moment\n",
    "\n",
    "P = (0.2, 0.1, 0.1, 0.1, 0.2, 0.3)\n",
    "n = 20\n",
    "k = 100\n",
    "\n",
    "X = multinomial(n, P, k)\n",
    "Y = X[:,1] + X[:,2] + X[:,3]\n",
    "Œº_Y = Y.mean()\n",
    "m3 = ((Y - Œº_Y) ** 3).mean()\n",
    "print(m3)\n",
    "print(moment(Y, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat Y as a Binom that counts the successes of $ X2 \\cup X3 \\cup X4 $, i.e $ Y\\sim Binom(n, p2+p3+p4) $\n",
    "In class we saw that the third moment of X-Binom(n,p) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "n * p * (1 - p) * (1 - 2*p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very different from one another because the emperical centralized third momentum is very sensible to the bias of sampling.\n",
    "The way to overcome this is to sample in bigger sizes and get closer to the theoretical calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100**3\n",
    "\n",
    "X = multinomial(n, P, k)\n",
    "Y = X[:,1] + X[:,2] + X[:,3]\n",
    "print(moment(Y, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Covariance and independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the variance of the sum X +Y + Z of three random variables in terms of the variances of X, Y and Z and the covariances between each pair of random variables? What happens if X,Y,Z are pairwise independent? If X,Y,Z are pairwise independent, are they necessarily collectively independent? Prove your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets note that\n",
    "$ Cov(X, Y+Z) = \\mathbb{E}[(X-\\mathbb{E}(X))*((Y-\\mathbb{E}(Y))+(Z-\\mathbb{E}(Z)))] = \\mathbb{E}[(X-\\mathbb{E}(X))*(Y-\\mathbb{E}(Y)))+((X-\\mathbb{E}(X))*(Z-\\mathbb{E}(Z)))] = \\mathbb{E}[(X-\\mathbb{E}(X))*(Y-\\mathbb{E}(Y)))]+\\mathbb{E}[((X-\\mathbb{E}(X))*(Z-\\mathbb{E}(Z)))] = Cov(X,Y)+Cov(X,Z) $\n",
    "\n",
    "And conclude\n",
    "\n",
    "$ V(X+Y+Z) = V((X+Y) + Z) = V(X+Y) + V(Z) + 2Cov(X+Y,Z)\n",
    " = V(X) + V(Y) + V(Z) + 2Cov(X,Z) + 2Cov(Y,Z) + 2Cov(X,Y) $\n",
    "\n",
    "in case X,Y,Z are pairwise independent then we get that each covariance pair is 0\n",
    "\n",
    "$ V(X+Y+Z) = V(X) + V(Y) + V(Z) + 2Cov(X,Z) + 2Cov(Y,Z) + 2Cov(X,Y) = V(X) + V(Y) + V(Z) $\n",
    "\n",
    "\n",
    "Pairwise independece doesn't imply collective independece\n",
    "Lets have $ X,Y ~ Ber(0.5) $ and $ Z=XOR(X,Y) $\n",
    "\n",
    "$ \\mathbb{P}(X=k) = \\mathbb{P}(Y=k) = \\mathbb{P}(Z=k) = \\frac{1}{2} $ for each $ k\\in[0,1] $\n",
    "And for each $ k,j\\in[0,1], {D_1\\\\},{D_2\\\\}\\in[X,Y,Z] $ where ${D_1\\\\}\\neq{D_2\\\\}$ We have $ \\mathbb{P}({D_1\\\\}=k,{D_2\\\\}=j)= \\frac{1}{4} = \\mathbb{P}({D_1\\\\}=k)*\\mathbb{P}({D_2\\\\}=j) $\n",
    "\n",
    "Which implise pairwise independence.\n",
    "But $ \\mathbb{P}(X=0,Y=0,Z=1) = 0 \\neq\\frac{1}{8}$ \n",
    "Which means they are not collectivly independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.A\n",
    "Write a program, `Q = NFoldConv(P , n)`, that takes as input:\n",
    "* A distribution, P, of a random variable that takes finitely many integer values\n",
    "* An integer n\n",
    "\n",
    "and produces the distribution, Q, of the sum of n independent repeats of random variables, each of which has the distribution P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "\n",
    "def NFoldConv(P, n):\n",
    "    if n == 1:\n",
    "        return P\n",
    "    P_n_1 = NFoldConv(P, n - 1)\n",
    "    \n",
    "    P_n = {}\n",
    "    for i in P.keys():\n",
    "        for j in P_n_1.keys():\n",
    "            if i + j in P_n.keys():\n",
    "                P_n[i + j] += P[i] * P_n_1[j] \n",
    "            else:\n",
    "                P_n[i + j] = P[i] * P_n_1[j]\n",
    "    return P_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.B\n",
    "Compute the distribution of the sum of the results of rolling a fair octahedron 17 times.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/Octahedron.jpg\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCT = dict(zip(range(1,9),[0.125]*8))\n",
    "results = NFoldConv(OCT, 17)\n",
    "results = np.asarray(list(results.items()))\n",
    "plt.plot(*results.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - Counting Similar Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a probaility space $(\\Omega, P)$:\n",
    "* $\\Omega = \\{0,1\\}^n$.\n",
    "* $P$ is induced by independantly tossing a $p$-coin ($p \\in [0,1]$) n times.\n",
    "\n",
    "For $\\omega \\in \\Omega$ let $W(\\omega) =$ number of 1s in $\\omega$.\n",
    "\n",
    "For $\\omega \\in \\Omega$ let the random variable $C = C_{p, n}$ be defined by:\n",
    "$$C(\\omega) = |\\{\\zeta : W(\\zeta)=W(\\omega)\\}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.A\n",
    "Plot the distribution of $W$ for $n = 100$ and $p = 0.3$. What is the name of this distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "n = 100\n",
    "p = 0.3\n",
    "\n",
    "def get_W(n,p):\n",
    "    Œ© = list(range(n + 1))\n",
    "    return dict(zip(Œ©, [binom.pmf(r, n, p) for r in Œ© ]))\n",
    "\n",
    "# Plotting Binomial distribution\n",
    "\n",
    "W = get_W(n, p)\n",
    "plt.bar(W.keys(), W.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.B\n",
    "State a formula for comuting $E(C)$.\n",
    "\n",
    "Compute $E(C)$ for $p=0.1, 0.5, 0.8$ and $n=10, 20, 50, 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_C(n, p):\n",
    "    W = get_W(n,p)\n",
    "    C = np.asarray([list(W.values()).count(W[w]) for w in W.keys()])\n",
    "    return sum(C * np.asarray(list(W.values())))\n",
    "\n",
    "Ps = [0.1, 0.5, 0.8]\n",
    "Ns = [10, 20, 50, 100]\n",
    "\n",
    "for p in Ps:\n",
    "    for n in Ns:\n",
    "        EC = expected_C(n, p)\n",
    "        print(\"p: %s n: %s E(C): %s\" % (p, n, EC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.C \n",
    "Plot the histograms of the values of $C$ for 1000 samples drawn from the space $(\\Omega, P)$ for each combination of $p$ and $n$ from the previous section. <br>\n",
    "Add text to each histogram with the empirical average of $C$ and the computed value of $E(C)$ (from the previous section). <br>\n",
    "In every histogram indicate the values of $n$ and $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "k = 1000\n",
    "for p in Ps:\n",
    "    for n in Ns:\n",
    "        W = binom.rvs(n, p, size=k)\n",
    "        Œ© = np.unique(W)\n",
    "        probs = np.asarray([np.count_nonzero(W==w) for w in Œ©])/k\n",
    "        W = dict(zip(Œ©, probs))\n",
    "        C = np.asarray([list(W.values()).count(W[w]) for w in W.keys()])\n",
    "        plt.hist(C)\n",
    "        plt.title(\"p: %s n: %s\" % (p, n))\n",
    "        plt.show()\n",
    "        results.append((C.mean(), expected_C(n, p)))\n",
    "        print(\"Emperical Average = %.2f E(C) = %.2f\" % results[-1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.D\n",
    "Use a scatter plot to compare the empirical and the computed values from the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*np.asarray(results).T, alpha=0.5)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to plot the emperical mean and the expected value on top eachother while emphasizing their size difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in Ps:\n",
    "    for n in Ns:\n",
    "        EC = expected_C(n, p)\n",
    "        \n",
    "        W = binom.rvs(n, p, size=k)\n",
    "        Œ© = np.unique(W)\n",
    "        probs = np.asarray([np.count_nonzero(W==w) for w in Œ©])/k\n",
    "        W = dict(zip(Œ©, probs))\n",
    "        C = np.asarray([list(W.values()).count(W[w]) for w in W.keys()])\n",
    "        \n",
    "        # Emperical Mean\n",
    "        scale=50\n",
    "        plt.scatter(p, n, s=scale**C.mean(), color=\"y\", alpha=.5)\n",
    "        # Expected Value\n",
    "        plt.scatter(p, n, s=scale**EC, color=\"m\", alpha=.5)\n",
    "        plt.gca().set_xlabel('p')\n",
    "        plt.gca().set_ylabel('n')\n",
    "        #plt.scatter(p, n, s=200*C.mean()/EC, alpha=.5)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
